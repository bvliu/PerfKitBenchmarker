{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"PerfKit Benchmarker User Manual","text":"<p>Welcome to the official documentation for PerfKit Benchmarker (PKB). This site is designed to help you navigate the tool, from your first benchmark to advanced configurations.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>If you are new to PKB, start with these guides:</p> <ul> <li>Installation: Setup PKB on your local machine or in a container.</li> <li>Basic Usage: Learn how to run your first benchmark on various cloud providers.</li> <li>Tutorials: Step-by-step walkthroughs for common scenarios.</li> </ul>"},{"location":"#advanced-documentation","title":"Advanced Documentation","text":"<p>For deeper dives into PKB's capabilities:</p> <ul> <li>Configuration: Understanding YAML configs and flag overrides.</li> <li>Benchmarks &amp; Licensing: Detailed list of supported benchmarks and their licenses.</li> <li>Publishers: How to export results to Elasticsearch or InfluxDB.</li> <li>Advanced Topics: Windows benchmarks, Juju, and non-cloud environments.</li> </ul>"},{"location":"#contribution","title":"Contribution","text":"<p>For information on how to contribute, please see our Internal Blueprint or the Roadmap.</p> <p>Looking for the \"Front Door\"? Return to the GitHub Repository.</p>"},{"location":"advanced/","title":"How to Run Windows Benchmarks","text":"<p>Install all dependencies as above and ensure that smbclient is installed on your system if you are running on a linux controller:</p> <pre><code>$ which smbclient\n/usr/bin/smbclient\n</code></pre> <p>Now you can run Windows benchmarks by running with <code>--os_type=windows</code>. Windows has a different set of benchmarks than Linux does. They can be found under <code>perfkitbenchmarker/windows_benchmarks/</code>. The target VM OS is Windows Server 2012 R2.</p>"},{"location":"advanced/#how-to-run-benchmarks-with-juju","title":"How to Run Benchmarks with Juju","text":"<p>Juju is a service orchestration tool that enables you to quickly model, configure, deploy and manage entire cloud environments. Supported benchmarks will deploy a Juju-modeled service automatically, with no extra user configuration required, by specifying the <code>--os_type=juju</code> flag.</p>"},{"location":"advanced/#example","title":"Example","text":"<pre><code>$ ./pkb.py --cloud=AWS --os_type=juju --benchmarks=cassandra_stress\n</code></pre>"},{"location":"advanced/#benchmark-support","title":"Benchmark support","text":"<p>Benchmark/Package authors need to implement the JujuInstall() method inside their package. This method deploys, configures, and relates the services to be benchmarked. Please note that other software installation and configuration should be bypassed when <code>FLAGS.os_type == JUJU</code>. See <code>perfkitbenchmarker/linux_packages/cassandra.py</code> for an example implementation.</p>"},{"location":"advanced/#advanced-how-to-run-benchmarks-without-cloud-provisioning-eg-local-workstation","title":"Advanced: How To Run Benchmarks Without Cloud Provisioning (e.g., local workstation)","text":"<p>It is possible to run PerfKit Benchmarker without running the Cloud provisioning steps. This is useful if you want to run on a local machine, or have a benchmark like iperf run from an external point to a Cloud VM.</p> <p>In order to do this you need to make sure:</p> <ul> <li>The static (i.e. not provisioned by PerfKit Benchmarker) machine is ssh'able</li> <li>The user PerfKitBenchmarker will login as has 'sudo' access. ( Note we     hope to remove this restriction soon )</li> </ul> <p>Next, you will want to create a YAML user config file describing how to connect to the machine as follows:</p> <pre><code>static_vms:\n  - &amp;vm1 # Using the &amp; character creates an anchor that we can\n         # reference later by using the same name and a * character.\n    ip_address: 170.200.60.23\n    user_name: voellm\n    ssh_private_key: /home/voellm/perfkitkeys/my_key_file.pem\n    zone: Siberia\n    disk_specs:\n      - mount_point: /data_dir\n</code></pre> <ul> <li>The <code>ip_address</code> is the address where you want benchmarks to run.</li> <li><code>ssh_private_key</code> is where to find the private ssh key.</li> <li><code>zone</code> can be anything you want. It is used when publishing results.</li> <li><code>disk_specs</code> is used by all benchmarks which use disk (i.e., <code>fio</code>,     <code>bonnie++</code>, many others).</li> </ul> <p>In the same file, configure any number of benchmarks (in this case just iperf), and reference the static VM as follows:</p> <pre><code>iperf:\n  vm_groups:\n    vm_1:\n      static_vms:\n        - *vm1\n</code></pre> <p>I called my file <code>iperf.yaml</code> and used it to run iperf from Siberia to a GCP VM in us-central1-f as follows:</p> <pre><code>$ ./pkb.py --benchmarks=iperf --machine_type=f1-micro --benchmark_config_file=iperf.yaml --zones=us-central1-f --ip_addresses=EXTERNAL\n</code></pre> <ul> <li><code>ip_addresses=EXTERNAL</code> tells PerfKit Benchmarker not to use 10.X (ie     Internal) machine addresses that all Cloud VMs have. Just use the external     IP address.</li> </ul> <p>If a benchmark requires two machines like iperf, you can have two machines in the same YAML file as shown below. This means you can indeed run between two machines and never provision any VMs in the Cloud.</p> <pre><code>static_vms:\n  - &amp;vm1\n    ip_address: &lt;ip1&gt;\n    user_name: connormccoy\n    ssh_private_key: /home/connormccoy/.ssh/google_compute_engine\n    internal_ip: 10.240.223.37\n    install_packages: false\n  - &amp;vm2\n    ip_address: &lt;ip2&gt;\n    user_name: connormccoy\n    ssh_private_key: /home/connormccoy/.ssh/google_compute_engine\n    internal_ip: 10.240.234.189\n    ssh_port: 2222\n\niperf:\n  vm_groups:\n    vm_1:\n      static_vms:\n        - *vm2\n    vm_2:\n      static_vms:\n        - *vm1\n</code></pre>"},{"location":"advanced/#integration-testing","title":"Integration Testing","text":"<p>If you wish to run unit or integration tests, ensure that you have <code>tox &gt;= 2.0.0</code> installed.</p> <p>In addition to regular unit tests, which are run via <code>hooks/check-everything</code>, PerfKit Benchmarker has integration tests, which create actual cloud resources and take time and money to run. For this reason, they will only run when the variable <code>PERFKIT_INTEGRATION</code> is defined in the environment. The command</p> <pre><code>$ tox -e integration\n</code></pre> <p>will run the integration tests. The integration tests depend on having installed and configured all of the relevant cloud provider SDKs, and will fail if you have not done so.</p>"},{"location":"benchmarks/","title":"Licensing","text":"<p>PerfKit Benchmarker provides wrappers and workload definitions around popular benchmark tools. We made it very simple to use and automate everything we can. It instantiates VMs on the Cloud provider of your choice, automatically installs benchmarks, and runs the workloads without user interaction.</p> <p>Due to the level of automation you will not see prompts for software installed as part of a benchmark run. Therefore you must accept the license of each of the benchmarks individually, and take responsibility for using them before you use the PerfKit Benchmarker.</p> <p>Moving forward, you will need to run PKB with the explicit flag --accept-licenses.</p> <p>In its current release these are the benchmarks that are executed:</p> <ul> <li><code>aerospike</code>:     Apache v2 for the client     and     GNU AGPL v3.0 for the server</li> <li><code>bonnie++</code>: GPL v2</li> <li><code>cassandra_ycsb</code>: Apache v2</li> <li><code>cassandra_stress</code>: Apache v2</li> <li><code>cloudsuite3.0</code>:     CloudSuite 3.0 license</li> <li><code>cluster_boot</code>: MIT License</li> <li><code>coremark</code>: EEMBC</li> <li><code>copy_throughput</code>: Apache v2</li> <li><code>fio</code>: GPL v2</li> <li><code>gpu_pcie_bandwidth</code>:     NVIDIA Software Licence Agreement</li> <li><code>hadoop_terasort</code>: Apache v2</li> <li><code>hpcc</code>: Original BSD license</li> <li><code>hpcg</code>:     BSD 3-clause</li> <li><code>iperf</code>:     UIUC License</li> <li><code>memtier_benchmark</code>:     GPL v2</li> <li><code>mesh_network</code>:     HP license</li> <li><code>mongodb</code>: Deprecated.     GNU AGPL v3.0</li> <li><code>mongodb_ycsb</code>: GNU AGPL v3.0</li> <li><code>multichase</code>:     Apache v2</li> <li><code>netperf</code>:     HP license</li> <li><code>oldisim</code>:     Apache v2</li> <li><code>object_storage_service</code>: Apache v2</li> <li><code>pgbench</code>: PostgreSQL Licence</li> <li><code>ping</code>: No license needed.</li> <li><code>silo</code>: MIT License</li> <li><code>scimark2</code>: public domain</li> <li><code>speccpu2006</code>: SPEC CPU2006</li> <li><code>SHOC</code>:     BSD 3-clause</li> <li><code>sysbench_oltp</code>: GPL v2</li> <li><code>TensorFlow</code>:     Apache v2</li> <li><code>tomcat</code>:     Apache v2</li> <li><code>unixbench</code>:     GPL v2</li> <li><code>wrk</code>:     Modified Apache v2</li> <li><code>ycsb</code> (used by <code>mongodb</code>,     <code>hbase_ycsb</code>, and others):     Apache v2</li> </ul> <p>Some of the benchmarks invoked require Java. You must also agree with the following license:</p> <ul> <li><code>openjdk-7-jre</code>:     GPL v2 with the Classpath Exception</li> </ul> <p>SPEC CPU2006 benchmark setup cannot be automated. SPEC requires that users purchase a license and agree with their terms and conditions. PerfKit Benchmarker users must manually download <code>cpu2006-1.2.iso</code> from the SPEC website, save it under the <code>perfkitbenchmarker/data</code> folder (e.g. <code>~/PerfKitBenchmarker/perfkitbenchmarker/data/cpu2006-1.2.iso</code>), and also supply a runspec cfg file (e.g. <code>~/PerfKitBenchmarker/perfkitbenchmarker/data/linux64-x64-gcc47.cfg</code>). Alternately, PerfKit Benchmarker can accept a tar file that can be generated with the following steps:</p> <ul> <li>Extract the contents of <code>cpu2006-1.2.iso</code> into a directory named <code>cpu2006</code></li> <li>Run <code>cpu2006/install.sh</code></li> <li>Copy the cfg file into <code>cpu2006/config</code></li> <li>Create a tar file containing the <code>cpu2006</code> directory, and place it under the     <code>perfkitbenchmarker/data</code> folder (e.g.     <code>~/PerfKitBenchmarker/perfkitbenchmarker/data/cpu2006v1.2.tgz</code>).</li> </ul> <p>PerfKit Benchmarker will use the tar file if it is present. Otherwise, it will search for the iso and cfg files.</p>"},{"location":"benchmarks/#preprovisioned-data","title":"Preprovisioned Data","text":"<p>As mentioned above, some benchmarks require preprovisioned data. This section describes how to preprovision this data.</p>"},{"location":"benchmarks/#benchmarks-with-preprovisioned-data","title":"Benchmarks with Preprovisioned Data","text":""},{"location":"benchmarks/#sample-preprovision-benchmark","title":"Sample Preprovision Benchmark","text":"<p>This benchmark demonstrates the use of preprovisioned data. Create the following file to upload using the command line:</p> <pre><code>echo \"1234567890\" &gt; preprovisioned_data.txt\n</code></pre> <p>To upload, follow the instructions below with a filename of <code>preprovisioned_data.txt</code> and a benchmark name of <code>sample</code>.</p>"},{"location":"benchmarks/#clouds-with-preprovisioned-data","title":"Clouds with Preprovisioned Data","text":""},{"location":"benchmarks/#google-cloud","title":"Google Cloud","text":"<p>To preprovision data on Google Cloud, you will need to upload each file to Google Cloud Storage using gsutil. First, you will need to create a storage bucket that is accessible from VMs created in Google Cloud by PKB. Then copy each file to this bucket using the command</p> <pre><code>gsutil cp &lt;filename&gt; gs://&lt;bucket&gt;/&lt;benchmark-name&gt;/&lt;filename&gt;\n</code></pre> <p>To run a benchmark on Google Cloud that uses the preprovisioned data, use the flag <code>--gcp_preprovisioned_data_bucket=&lt;bucket&gt;</code>.</p>"},{"location":"benchmarks/#aws","title":"AWS","text":"<p>To preprovision data on AWS, you will need to upload each file to S3 using the AWS CLI. First, you will need to create a storage bucket that is accessible from VMs created in AWS by PKB. Then copy each file to this bucket using the command</p> <pre><code>aws s3 cp &lt;filename&gt; s3://&lt;bucket&gt;/&lt;benchmark-name&gt;/&lt;filename&gt;\n</code></pre> <p>To run a benchmark on AWS that uses the preprovisioned data, use the flag <code>--aws_preprovisioned_data_bucket=&lt;bucket&gt;</code>.</p>"},{"location":"configuration/","title":"Useful Global Flags","text":"<p>The following are some common flags used when configuring PerfKit Benchmarker.</p> Flag Notes <code>--helpmatch=pkb</code> see all global flags <code>--helpmatch=hpcc</code> see all flags associated with the hpcc benchmark. You :                    : can substitute any benchmark name to see the          : :                    : associated flags.                                     : <code>--benchmarks</code> A comma separated list of benchmarks or benchmark :                    : sets to run such as <code>--benchmarks=iperf,ping</code> . To    : :                    : see the full list, run `./pkb.py                      : :                    : --helpmatch=benchmarks                                : <code>--cloud</code> Cloud where the benchmarks are run. See the table :                    : below for choices.                                    : <code>--machine_type</code> Type of machine to provision if pre-provisioned :                    : machines are not used. Most cloud providers accept    : :                    : the names of pre-defined provider-specific machine    : :                    : types (for example, GCP supports                      : :                    : <code>--machine_type=n1-standard-8</code> for a GCE              : :                    : n1-standard-8 VM). Some cloud providers support YAML  : :                    : expressions that match the corresponding VM spec      : :                    : machine_type property in the [YAML                    : :                    : configs](#configurations-and-configuration-overrides) : :                    : (for example, GCP supports `--machine_type=\"{cpus\\:   : :                    : 1, memory\\: 4.5GiB}\"` for a GCE custom VM with 1 vCPU : :                    : and 4.5GiB memory). Note that the value provided by   : :                    : this flag will affect all provisioned machines; users : :                    : who wish to provision different machine types for     : :                    : different roles within a single benchmark run should  : :                    : use the [YAML                                         : :                    : configs](#configurations-and-configuration-overrides) : :                    : for finer control.                                    : <code>--zones</code> This flag allows you to override the default zone. :                    : See the table below.                                  : <code>--data_disk_type</code> Type of disk to use. Names are provider-specific, but :                    : see table below.                                      : <p>The default cloud is 'GCP', override with the <code>--cloud</code> flag. Each cloud has a default zone which you can override with the <code>--zones</code> flag, the flag supports the same values that the corresponding Cloud CLIs take:</p> Cloud name Default zone Notes GCP us-central1-a AWS us-east-1a Azure eastus2 IBMCloud us-south-1 AliCloud West US DigitalOcean sfo1 You must use a zone that supports the :              :               : features 'metadata' (for cloud config) and  : :              :               : 'private_networking'.                       : OpenStack nova CloudStack QC-1 Rackspace IAD OnMetal machine-types are available only in :              :               : IAD zone                                    : Kubernetes k8s ProfitBricks AUTO Additional zones: ZONE_1, ZONE_2, or ZONE_3 <p>Example:</p> <pre><code>./pkb.py --cloud=GCP --zones=us-central1-a --benchmarks=iperf,ping\n</code></pre> <p>The disk type names vary by provider, but the following table summarizes some useful ones. (Many cloud providers have more disk types beyond these options.)</p> Cloud name Network-attached SSD Network-attached HDD GCP pd-ssd pd-standard AWS gp3 st1 Azure Premium_LRS Standard_LRS Rackspace cbs-ssd cbs-sata <p>Also note that <code>--data_disk_type=local</code> tells PKB not to allocate a separate disk, but to use whatever comes with the VM. This is useful with AWS instance types that come with local SSDs, or with the GCP <code>--gce_num_local_ssds</code> flag.</p> <p>If an instance type comes with more than one disk, PKB uses whichever does not hold the root partition. Specifically, on Azure, PKB always uses <code>/dev/sdb</code> as its scratch device.</p>"},{"location":"configuration/#proxy-configuration-for-vm-guests","title":"Proxy configuration for VM guests.","text":"<p>If the VM guests do not have direct Internet access in the cloud environment, you can configure proxy settings through <code>pkb.py</code> flags.</p> <p>To do that simple setup three flags (All urls are in notation ): The flag values use the same <code>&lt;protocol&gt;://&lt;server&gt;:&lt;port&gt;</code> syntax as the corresponding environment variables, for example <code>--http_proxy=http://proxy.example.com:8080</code> .</p> Flag Notes <code>--http_proxy</code> Needed for package manager on Guest OS and for some :                 : Perfkit packages                                        : <code>--https_proxy</code> Needed for package manager or Ubuntu guest and for from :                 : Github downloaded packages                              : <code>--ftp_proxy</code> Needed for some Perfkit packages"},{"location":"configuration/#configurations-and-configuration-overrides","title":"Configurations and Configuration Overrides","text":"<p>Each benchmark now has an independent configuration which is written in YAML. Users may override this default configuration by providing a configuration. This allows for much more complex setups than previously possible, including running benchmarks across clouds.</p> <p>A benchmark configuration has a somewhat simple structure. It is essentially just a series of nested dictionaries. At the top level, it contains VM groups. VM groups are logical groups of homogenous machines. The VM groups hold both a <code>vm_spec</code> and a <code>disk_spec</code> which contain the parameters needed to create members of that group. Here is an example of an expanded configuration:</p> <pre><code>hbase_ycsb:\n  vm_groups:\n    loaders:\n      vm_count: 4\n      vm_spec:\n        GCP:\n          machine_type: n1-standard-1\n          image: ubuntu-16-04\n          zone: us-central1-c\n        AWS:\n          machine_type: m3.medium\n          image: ami-######\n          zone: us-east-1a\n        # Other clouds here...\n      # This specifies the cloud to use for the group. This allows for\n      # benchmark configurations that span clouds.\n      cloud: AWS\n      # No disk_spec here since these are loaders.\n    master:\n      vm_count: 1\n      cloud: GCP\n      vm_spec:\n        GCP:\n          machine_type:\n            cpus: 2\n            memory: 10.0GiB\n          image: ubuntu-16-04\n          zone: us-central1-c\n        # Other clouds here...\n      disk_count: 1\n      disk_spec:\n        GCP:\n          disk_size: 100\n          disk_type: standard\n          mount_point: /scratch\n        # Other clouds here...\n    workers:\n      vm_count: 4\n      cloud: GCP\n      vm_spec:\n        GCP:\n          machine_type: n1-standard-4\n          image: ubuntu-16-04\n          zone: us-central1-c\n        # Other clouds here...\n      disk_count: 1\n      disk_spec:\n        GCP:\n          disk_size: 500\n          disk_type: remote_ssd\n          mount_point: /scratch\n        # Other clouds here...\n</code></pre> <p>For a complete list of keys for <code>vm_spec</code>s and <code>disk_spec</code>s see <code>virtual_machine_spec.BaseVmSpec</code> and <code>disk.BaseDiskSpec</code> and their derived classes.</p> <p>User configs are applied on top of the existing default config and can be specified in two ways. The first is by supplying a config file via the <code>--benchmark_config_file</code> flag. The second is by specifying a single setting to override via the <code>--config_override</code> flag.</p> <p>A user config file only needs to specify the settings which it is intended to override. For example if the only thing you want to do is change the number of VMs for the <code>cluster_boot</code> benchmark, this config is sufficient:</p> <pre><code>cluster_boot:\n  vm_groups:\n    default:\n      vm_count: 100\n</code></pre> <p>You can achieve the same effect by specifying the <code>--config_override</code> flag. The value of the flag should be a path within the YAML (with keys delimited by periods), an equals sign, and finally the new value:</p> <pre><code>--config_override=cluster_boot.vm_groups.default.vm_count=100\n</code></pre> <p>See the section below for how to use static (i.e. pre-provisioned) machines in your config.</p>"},{"location":"configuration/#specifying-flags-in-configuration-files","title":"Specifying Flags in Configuration Files","text":"<p>You can now specify flags in configuration files by using the <code>flags</code> key at the top level in a benchmark config. The expected value is a dictionary mapping flag names to their new default values. The flags are only defaults; it's still possible to override them via the command line. It's even possible to specify conflicting values of the same flag in different benchmarks:</p> <pre><code>iperf:\n  flags:\n    machine_type: n1-standard-2\n    zone: us-central1-b\n    iperf_sending_thread_count: 2\n\nnetperf:\n  flags:\n    machine_type: n1-standard-8\n</code></pre> <p>The new defaults will only apply to the benchmark in which they are specified.</p>"},{"location":"installation/","title":"Installation and Setup","text":"<p>Before you can run the PerfKit Benchmarker, you need account(s) on the cloud provider(s) you want to benchmark (see providers). You also need the software dependencies, which are mostly command line tools and credentials to access your accounts without a password. The following steps should help you get up and running with PKB.</p>"},{"location":"installation/#python-3","title":"Python 3","text":"<p>The recommended way to install and run PKB is in a virtualenv with the latest version of Python 3 (at least Python 3.12). Most Linux distributions and recent Mac OS X versions already have Python 3 installed at <code>/usr/bin/python3</code>.</p> <p>If Python is not installed, you can likely install it using your distribution's package manager, or see the Python Download page.</p> <pre><code># install pyenv to install python on persistent home directory\ncurl https://pyenv.run | bash\n\n# add to path\necho 'export PATH=\"$HOME/.pyenv/bin:$PATH\"' &gt;&gt; ~/.bashrc\necho 'eval \"$(pyenv init -)\"' &gt;&gt; ~/.bashrc\necho 'eval \"$(pyenv virtualenv-init -)\"' &gt;&gt; ~/.bashrc\n\n# update bashrc\nsource ~/.bashrc\n\n# install python 3.12 and make default\npyenv install 3.12\npyenv global 3.12\n</code></pre>"},{"location":"installation/#install-perfkit-benchmarker","title":"Install PerfKit Benchmarker","text":"<p>Download the latest PerfKit Benchmarker release from GitHub. You can also clone the working version with:</p> <pre><code>$ cd $HOME\n$ git clone https://github.com/GoogleCloudPlatform/PerfKitBenchmarker.git\n</code></pre> <p>Install Python library dependencies:</p> <pre><code>$ pip3 install -r $HOME/PerfKitBenchmarker/requirements.txt\n</code></pre> <p>You may need to install additional dependencies depending on the cloud provider you are using. For example, for AWS:</p> <pre><code>$ cd $HOME/PerfKitBenchmarker/perfkitbenchmarker/providers/aws\n$ pip3 install -r requirements.txt\n</code></pre>"},{"location":"publishers/","title":"Using Elasticsearch Publisher","text":"<p>PerfKit data can optionally be published to an Elasticsearch server. To enable this, the <code>elasticsearch</code> Python package must be installed.</p> <pre><code>$ pip install elasticsearch\n</code></pre> <p>Note: The <code>elasticsearch</code> Python library and Elasticsearch must have matching major versions.</p> <p>The following are flags used by the Elasticsearch publisher. At minimum, all that is needed is the <code>--es_uri</code> flag.</p> Flag Notes <code>--es_uri</code> The Elasticsearch server address and port (e.g. :              : localhost\\:9200)                                          : <code>--es_index</code> The Elasticsearch index name to store documents (default: :              : perfkit)                                                  : <code>--es_type</code> The Elasticsearch document type (default: result) <p>Note: Amazon ElasticSearch service currently does not support transport on port 9200 therefore you must use endpoint with port 80 eg. <code>search-&lt;ID&gt;.es.amazonaws.com:80</code> and allow your IP address in the cluster.</p>"},{"location":"publishers/#using-influxdb-publisher","title":"Using InfluxDB Publisher","text":"<p>No additional packages need to be installed in order to publish Perfkit data to an InfluxDB server.</p> <p>InfluxDB Publisher takes in the flags for the Influx uri and the Influx DB name. The publisher will default to the pre-set defaults, identified below, if no uri or DB name is set. However, the user is required to at the very least call the <code>--influx_uri</code> flag to publish data to Influx.</p> Flag Notes Default <code>--influx_uri</code> The Influx DB address and port. localhost:8086 :                    : Expects the format hostname\\:port   :                : <code>--influx_db_name</code> The name of Influx DB database that perfkit :                    : you wish to publish to or create    :                :"},{"location":"tutorials/","title":"PerfKit Benchmarker Tutorials","text":"<ul> <li>First time end-to-end walk-through for beginners</li> <li>Google BigTable performance benchmarking tutorial</li> <li>Instructions for reproducing inter-region benchmark reports</li> </ul>"},{"location":"usage/","title":"Running a Single Benchmark","text":"<p>PerfKit Benchmarker can run benchmarks both on Cloud Providers (GCP, AWS, Azure, DigitalOcean) as well as any \"machine\" you can SSH into.</p>"},{"location":"usage/#example-run-on-gcp","title":"Example run on GCP","text":"<pre><code>$ ./pkb.py --project=&lt;GCP project ID&gt; --benchmarks=iperf --machine_type=f1-micro\n</code></pre>"},{"location":"usage/#example-run-on-aws","title":"Example run on AWS","text":"<pre><code>$ cd PerfKitBenchmarker\n$ ./pkb.py --cloud=AWS --benchmarks=iperf --machine_type=t2.micro\n</code></pre>"},{"location":"usage/#example-run-on-azure","title":"Example run on Azure","text":"<pre><code>$ ./pkb.py --cloud=Azure --machine_type=Standard_A0 --benchmarks=iperf\n</code></pre>"},{"location":"usage/#example-run-on-ibmcloud","title":"Example run on IBMCloud","text":"<pre><code>$ ./pkb.py --cloud=IBMCloud --machine_type=cx2-4x8 --benchmarks=iperf\n</code></pre>"},{"location":"usage/#example-run-on-alicloud","title":"Example run on AliCloud","text":"<pre><code>$ ./pkb.py --cloud=AliCloud --machine_type=ecs.s2.large --benchmarks=iperf\n</code></pre>"},{"location":"usage/#example-run-on-digitalocean","title":"Example run on DigitalOcean","text":"<pre><code>$ ./pkb.py --cloud=DigitalOcean --machine_type=16gb --benchmarks=iperf\n</code></pre>"},{"location":"usage/#example-run-on-openstack","title":"Example run on OpenStack","text":"<pre><code>$ ./pkb.py --cloud=OpenStack --machine_type=m1.medium \\\n           --openstack_network=private --benchmarks=iperf\n</code></pre>"},{"location":"usage/#example-run-on-kubernetes","title":"Example run on Kubernetes","text":"<pre><code>$ ./pkb.py --vm_platform=Kubernetes --benchmarks=iperf \\\n           --kubeconfig=/path/to/kubeconfig --use_k8s_vm_node_selectors=False\n</code></pre> <p>Note that this requires an existing Kubernetes cluster which you have the kubeconfig for. To spin up a cluster from scratch, see the GKE tutorial.</p>"},{"location":"usage/#example-run-on-mesos","title":"Example run on Mesos","text":"<pre><code>$ ./pkb.py --cloud=Mesos --benchmarks=iperf --marathon_address=localhost:8080\n</code></pre>"},{"location":"usage/#example-run-on-cloudstack","title":"Example run on CloudStack","text":"<pre><code>./pkb.py --cloud=CloudStack --benchmarks=ping --cs_network_offering=DefaultNetworkOffering\n</code></pre>"},{"location":"usage/#example-run-on-rackspace","title":"Example run on Rackspace","text":"<pre><code>$ ./pkb.py --cloud=Rackspace --machine_type=general1-2 --benchmarks=iperf\n</code></pre>"},{"location":"usage/#example-run-on-profitbricks","title":"Example run on ProfitBricks","text":"<pre><code>$ ./pkb.py --cloud=ProfitBricks --machine_type=Small --benchmarks=iperf\n</code></pre>"},{"location":"usage/#how-to-run-all-standard-benchmarks","title":"How to Run All Standard Benchmarks","text":"<p>Run with <code>--benchmarks=\"standard_set\"</code> and every benchmark in the standard set will run serially which can take a couple of hours. Additionally, if you don't specify <code>--cloud=...</code>, all benchmarks will run on the Google Cloud Platform.</p>"},{"location":"usage/#how-to-run-all-benchmarks-in-a-named-set","title":"How to Run All Benchmarks in a Named Set","text":"<p>Named sets are are groupings of one or more benchmarks in the benchmarking directory. This feature allows parallel innovation of what is important to measure in the Cloud, and is defined by the set owner. For example the GoogleSet is maintained by Google, whereas the StanfordSet is managed by Stanford. Once a quarter a meeting is held to review all the sets to determine what benchmarks should be promoted to the <code>standard_set</code>. The Standard Set is also reviewed to see if anything should be removed. To run all benchmarks in a named set, specify the set name in the benchmarks parameter (e.g., <code>--benchmarks=\"standard_set\"</code>). Sets can be combined with individual benchmarks or other named sets.</p>"},{"location":"usage/#running-selective-stages-of-a-benchmark","title":"Running selective stages of a benchmark","text":"<p>This procedure demonstrates how to run only selective stages of a benchmark. This technique can be useful for examining a machine after it has been prepared, but before the benchmark runs.</p> <p>This example shows how to provision and prepare the <code>cluster_boot</code> benchmark without actually running the benchmark.</p> <ol> <li> <p>Change to your local version of PKB: <code>cd $HOME/PerfKitBenchmarker</code></p> </li> <li> <p>Run provision, prepare, and run stages of <code>cluster_boot</code>.</p> <pre><code>./pkb.py --benchmarks=cluster_boot --machine_type=n1-standard-2 --zones=us-central1-f --run_stage=provision,prepare,run\n</code></pre> </li> <li> <p>The output from the console will tell you the run URI for your benchmark.     Try to ssh into the VM. The machine \"Default-0\" came from the VM group which     is specified in the benchmark_config for cluster_boot.</p> <pre><code>ssh -F /tmp/perfkitbenchmarker/runs/&lt;run_uri&gt;/ssh_config default-0\n</code></pre> </li> <li> <p>Now that you have examined the machines, teardown the instances that were     made and cleanup.</p> <pre><code>./pkb.py --benchmarks=cluster_boot --run_stage=teardown -run_uri=&lt;run_uri&gt;\n</code></pre> </li> </ol>"},{"location":"internal/contribution-standards/","title":"Contribution Standards","text":"<p>This page is a mirror of the project's CONTRIBUTING.md.</p> <p>Want to contribute? Great! First, read this page (including the small print at the end).</p>"},{"location":"internal/contribution-standards/#before-you-contribute","title":"Before you contribute","text":"<p>Before we can use your code, you must sign the Google Individual Contributor License Agreement (CLA), which you can do online. The CLA is necessary mainly because you own the copyright to your changes, even after your contribution becomes part of our codebase, so we need your permission to use and distribute your code. We also need to be sure of various other things\u2014for instance that you'll tell us if you know that your code infringes on other people's patents. You don't have to sign the CLA until after you've submitted your code for review and a member has approved it, but you must do it before we can put your code into our codebase. Before you start working on a larger contribution, you should get in touch with us first through the issue tracker with your idea so that we can help out and possibly guide you. Coordinating up front makes it much easier to avoid frustration later on.</p>"},{"location":"internal/contribution-standards/#code-reviews","title":"Code reviews","text":"<p>All submissions, including submissions by project members, require review. We use GitHub pull requests for this purpose.</p>"},{"location":"internal/contribution-standards/#configuring-a-local-development-environment","title":"Configuring a local development environment","text":"<p>Reference the setup section in the README or the beginner tutorial for an example of how to setup a local development environment.</p>"},{"location":"internal/contribution-standards/#fork-the-repository","title":"Fork the repository","text":"<p>If you plan on contributing, create your own fork of PerfKitBenchmarker to publish your changes. GitHub has a great tutorial (we follow the Fork &amp; pull model).</p> <ol> <li> <p>Fork the repository:</p> <pre><code>gh repo fork GoogleCloudPlatform/PerfKitBenchmarker\n</code></pre> </li> <li> <p>Clone the repository:</p> <pre><code>cd $HOME &amp;&amp; git clone https://github.com/{github_username}/PerfKitBenchmarker\n</code></pre> <pre><code>cd PerfKitBenchmarker/\n</code></pre> </li> <li> <p>Install Python 3 and pip (activate virtualenvs if needed).</p> </li> <li> <p>Install PerfKitBenchmarker's Python dependencies:</p> <pre><code>pip install -r requirements.txt\n</code></pre> </li> <li> <p>Install PerfKitBenchmarker's test dependencies:</p> <pre><code>pip install -r requirements-testing.txt\n</code></pre> </li> </ol>"},{"location":"internal/contribution-standards/#create-a-branch-and-make-changes","title":"Create a branch and make changes","text":"<p>Start from the master branch of the repository. This is the default.</p> <p>NOTE: If you plan on making many contributions to PKB, ask in your first PR for an invite to the collaborators list. After accepting the invite, you will be able to invoke the Cloud Build integration tests by adding \"/gcbrun\" as a comment to your PR.</p> <ol> <li> <p>Create a branch to contain your changes.</p> <pre><code>git checkout -b &lt;your-branch-name&gt;\n</code></pre> </li> <li> <p>Make your modifications to the code in one or more commits     (guidelines for useful git commit messages).</p> </li> <li> <p>Run unit tests and make sure they succeed:</p> <pre><code>  set -Eeuo pipefail; for test in $(find tests/ | grep \"test.py\"); do echo ; echo \"Running $test ...\";  python -m unittest $test -v ; done\n</code></pre> </li> <li> <p>For a single test, run</p> <pre><code>  python -m unittest '{test_file_path}' -v\n</code></pre> </li> <li> <p>Run the pyink formatter on the file(s) that were changed.</p> <p>Check the changed file for formatting without changing the file:</p> <pre><code>  pyink --pyink-indentation 2 --pyink-use-majority-quotes --unstable --line-length=80 --check --diff {file_path}\n</code></pre> <p>Format the file:</p> <pre><code>  pyink --pyink-indentation 2 --pyink-use-majority-quotes --unstable --line-length=80 {file_path}\n</code></pre> </li> <li> <p>Run lint-diffs on your changes.</p> <pre><code>git diff -U0 origin | lint-diffs\n</code></pre> <p>This will likely give some errors like:</p> <pre><code>elastic_kubernetes_service.py:1032:0: C0301: Line too long (120/80) (line-too-long)\n</code></pre> <p>Address all these errors before sending out the PR (&amp; after making changes to the PR).</p> </li> <li> <p>Update the appropriate sections in CHANGES.next.md with a summary of your     changes. For example, under \"Bug fixes and maintenance updates\" you might     put something like: <code>- Fix crazy bug X (GH-&lt;insert PR number here&gt; from     @&lt;insert your username here&gt;)</code></p> </li> <li> <p>Push your changes to GitHub</p> <pre><code>git push origin &lt;your-branch-name&gt;\n</code></pre> </li> <li> <p>Create a pull request to integrate your change into the <code>master</code> branch.     This can be done on github.com, or through CLIs. See GitHub's documentation     here     for more details. When making a pull request on GitHub, the base branch of     the Pull Request should be <code>master</code>. This is usually the default.</p> </li> </ol>"},{"location":"internal/contribution-standards/#developer-documentation","title":"Developer documentation","text":"<p>We have added a lot of comments into the code to make it easy to;</p> <ul> <li>Add new benchmarks (eg: --benchmarks=) <li>Add new package/os type support (eg: --os_type=) <li>Add new providers (eg: --cloud=) <li>etc...</li> <p>Even with lots of comments we make to support more detailed documentation. You will find the documentation we have on the Wiki pages. Missing documentation you want? Start a page and/or open an issue to get it added.</p>"},{"location":"internal/contribution-standards/#updating-the-github-pages-site","title":"Updating the GitHub Pages site:","text":"<p>The site requires Jekyll in order to run. Use <code>bundle install</code> and <code>bundle update</code> to update the dependencies in <code>Gemfile.lock</code>. <code>bundle exec jekyll serve</code> can be used to test the site locally.</p>"},{"location":"internal/contribution-standards/#the-small-print","title":"The small print","text":"<p>Contributions made by corporations are covered by a different agreement than the one above, the Software Grant and Corporate Contributor License Agreement. ~~~</p>"},{"location":"internal/roadmap/","title":"Internal Blueprint &amp; Roadmap","text":""},{"location":"internal/roadmap/#development-guide","title":"Development Guide","text":"<p>First start with the CONTRIBUTING.md file. It has the basics on how to work with PerfKitBenchmarker, and how to submit your pull requests.</p> <p>We have added a lot of comments into the code to make it easy to:</p> <ul> <li>Add new benchmarks (e.g.: <code>--benchmarks=&lt;new benchmark&gt;</code>)</li> <li>Add new package/os type support (e.g.: <code>--os_type=&lt;new os type&gt;</code>)</li> <li>Add new providers (e.g.: <code>--cloud=&lt;new provider&gt;</code>)</li> <li>etc.</li> </ul> <p>Even with lots of comments we make to support more detailed documention. You will find the documentation we have on the wiki. Missing documentation you want? Start a page and/or open an issue to get it added.</p>"},{"location":"internal/roadmap/#roadmap-planned-improvements","title":"Roadmap: Planned Improvements","text":"<p>Many... please add new requests via GitHub issues.</p>"}]}